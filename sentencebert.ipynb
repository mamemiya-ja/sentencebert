{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. BERT Tokenizerを用いて単語分割・IDへ変換\n",
    "## Tokenizerの準備\n",
    "import numpy as np \n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from mlflow import log_metric, log_param, log_artifact\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n",
    "import pytorch_lightning as pl\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "import torch\n",
    "\n",
    "max_length = 209\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "modelname = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "class SentenceBERT(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_name, lr):\n",
    "        # model_name: Transformersのモデルの名前\n",
    "        # lr: 学習率\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 例えば、self.hparams.lrでlrにアクセスできる。\n",
    "        # チェックポイント作成時にも自動で保存される。\n",
    "        self.save_hyperparameters() \n",
    "\n",
    "        # BERTのロード\n",
    "        self.bert_sc1 = BertModel.from_pretrained(\n",
    "            model_name\n",
    "        )\n",
    "        self.bert_sc1.cuda(1)\n",
    "\n",
    "        self.bert_sc2 = BertModel.from_pretrained(\n",
    "            model_name\n",
    "        )\n",
    "        self.bert_sc2.cuda(1)\n",
    "\n",
    "        self.triplet_loss = torch.nn.TripletMarginWithDistanceLoss(distance_function=torch.nn.PairwiseDistance(p=2), margin=1.0)\n",
    "        #self.triplet_loss = torch.nn.TripletMarginWithDistanceLoss(distance_function=torch.nn.CosineSimilarity(), margin=1.0)\n",
    "        self.cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "    # 学習データのミニバッチ(`batch`)が与えられた時に損失を出力する関数を書く。\n",
    "    # batch_idxはミニバッチの番号であるが今回は使わない。\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output1 = mean_pooling(self.bert_sc1(attention_mask=batch['attention_mask_a'], \n",
    "                                             input_ids=batch['input_ids_a'], \n",
    "                                             token_type_ids=batch['token_type_ids_a']), \n",
    "                               batch['attention_mask_a'])\n",
    "        output2 = mean_pooling(self.bert_sc2(attention_mask=batch['attention_mask_p'], \n",
    "                                             input_ids=batch['input_ids_p'], \n",
    "                                             token_type_ids=batch['token_type_ids_p']), \n",
    "                               batch['attention_mask_p'])\n",
    "        output3 = mean_pooling(self.bert_sc2(attention_mask=batch['attention_mask_n'], \n",
    "                                             input_ids=batch['input_ids_n'], \n",
    "                                             token_type_ids=batch['token_type_ids_n']), \n",
    "                               batch['attention_mask_n'])\n",
    "        loss = self.triplet_loss(output1,output2,output3)\n",
    "        self.log('train_loss', loss) # 損失を'train_loss'の名前でログをとる。\n",
    "        return loss\n",
    "\n",
    "    # 検証データのミニバッチが与えられた時に、\n",
    "    # 検証データを評価する指標を計算する関数を書く。\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output1 = mean_pooling(self.bert_sc1(attention_mask=batch['attention_mask_a'], \n",
    "                                             input_ids=batch['input_ids_a'], \n",
    "                                             token_type_ids=batch['token_type_ids_a']), \n",
    "                               batch['attention_mask_a'])\n",
    "        output2 = mean_pooling(self.bert_sc2(attention_mask=batch['attention_mask_p'], \n",
    "                                             input_ids=batch['input_ids_p'], \n",
    "                                             token_type_ids=batch['token_type_ids_p']), \n",
    "                               batch['attention_mask_p'])\n",
    "        output3 = mean_pooling(self.bert_sc2(attention_mask=batch['attention_mask_n'], \n",
    "                                             input_ids=batch['input_ids_n'], \n",
    "                                             token_type_ids=batch['token_type_ids_n']), \n",
    "                               batch['attention_mask_n'])\n",
    "        val_loss = self.triplet_loss(output1,output2,output3)\n",
    "        self.log('val_loss', val_loss) # 損失を'val_loss'の名前でログをとる。\n",
    "\n",
    "\n",
    "    # 学習に用いるオプティマイザを返す関数を書く。\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "# 6-20\n",
    "# PyTorch Lightningモデルのロード\n",
    "model = SentenceBERT.load_from_checkpoint(\n",
    "    './model/epoch=4-step=6396.ckpt'\n",
    ") \n",
    "\n",
    "model.cuda(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ast import literal_eval\n",
    "import pickle\n",
    "\n",
    "def calc_similarity(text):\n",
    "    \n",
    "    f = open(\"./sentenceVector.txt\", \"rb\")\n",
    "    vector_lst = pickle.load(f)\n",
    "\n",
    "    with open(\"../../data/PoliInfo3-FormalRun-FactVerification/Pref13_tokyo.json\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    #utterancesummaryの文ベクトルを出す\n",
    "    utterancesummary = text\n",
    "    encoding_ = tokenizer(\n",
    "        utterancesummary,\n",
    "        max_length=209,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    encoding_ = {k: v.cuda(1) for k, v in encoding_.items()}\n",
    "    with torch.no_grad():\n",
    "        output1 = mean_pooling(model.bert_sc1(attention_mask=encoding_['attention_mask'], \n",
    "                                    input_ids=encoding_['input_ids'], \n",
    "                                    token_type_ids=encoding_['token_type_ids']), \n",
    "                    encoding_['attention_mask'])\n",
    "\n",
    "    sim_diffs = []\n",
    "    #utterancesummaryの文ベクトルと議事録の各文の文ベクトルのcos類似度を取る    \n",
    "    for i in range(len(vector_lst)):\n",
    "        output2 = vector_lst[i]\n",
    "        cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        sim_diff = cos(output1, output2)\n",
    "        sim_diff = sim_diff.to('cpu').detach().numpy().copy()\n",
    "        sim_diffs.extend(np.array(sim_diff))\n",
    "    \n",
    "    #上位n件の要素を取ってくる。\n",
    "    n = 7\n",
    "    np_simdiffs = np.array(sim_diffs)\n",
    "    np_simdiffs = np.argsort(-np_simdiffs)[:n]\n",
    "    return np_simdiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_open = open('../../data/PoliInfo3-FormalRun-FactVerification/PoliInfo3_FactVerification_Formal_Train.json', 'r')\n",
    "json_load = json.load(json_open)\n",
    "\n",
    "correct_num = 0\n",
    "num = 0\n",
    "#valuesで値をとってくる\n",
    "for v in json_load:\n",
    "    if v[\"DocumentEntailment\"] != False:\n",
    "        np_simdiffs = calc_similarity(v[\"UtteranceSummary\"])\n",
    "        startingline = v[\"StartingLine\"]\n",
    "        endingline = v[\"EndingLine\"]\n",
    "        for i in np_simdiffs:\n",
    "            num += 1\n",
    "            for j in range(startingline, endingline+1):\n",
    "                if i+1 == j:\n",
    "                    correct_num += 1\n",
    "                    \n",
    "accuracy = correct_num / num\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv3': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a20c0a6a371a57aad6261c688c7be000c4ef211514c80fb72f7d0b4a9edecbe6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
